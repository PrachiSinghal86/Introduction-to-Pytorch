{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion-MNIST",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrachiSinghal86/Introduction-to-Pytorch/blob/master/Fashion_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo5K5gJrvE_k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "2232d860-0d79-43ae-c8e5-1dadd9990067"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import helper\n",
        "import matplotlib.pyplot as ply\n",
        "from torchvision import datasets,transforms\n",
        "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,)),])\n",
        "#load trainset\n",
        "trainset=datasets.FashionMNIST('Fashion-MNIST_data/',download=True,train=True,transform=transform)\n",
        "trainloader=torch.utils.data.DataLoader(trainset,batch_size=64,shuffle=True)\n",
        "#load test set\n",
        "testset=datasets.FashionMNIST('Fashion-MNIST_data/',download=True,train=False,transform=transform)\n",
        "testloader=torch.utils.data.DataLoader(testset,batch_size=64,shuffle=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/26421880 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to Fashion-MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "26427392it [00:00, 67993635.20it/s]                              \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting Fashion-MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz to Fashion-MNIST_data/FashionMNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 460833.40it/s]\n",
            "  2%|▏         | 98304/4422102 [00:00<00:04, 884621.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to Fashion-MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting Fashion-MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to Fashion-MNIST_data/FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to Fashion-MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4423680it [00:00, 22733131.07it/s]                         \n",
            "8192it [00:00, 158416.46it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting Fashion-MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to Fashion-MNIST_data/FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to Fashion-MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting Fashion-MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to Fashion-MNIST_data/FashionMNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HQ1NVjawBUO",
        "colab_type": "code",
        "outputId": "ba806ebb-8573-4102-b31a-fdcf785cbdfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "images, labels = next(iter(trainloader))  \n",
        "images = images.view(images.shape[0], -1)\n",
        "print(images.shape)\n",
        "model =nn.Sequential(nn.Linear(784,256),\n",
        "                     nn.ReLU(),nn.Linear(256,128),\n",
        "                     nn.ReLU(),nn.Linear(128,64),\n",
        "                     \n",
        "                     nn.ReLU(),nn.Linear(64,10),nn.LogSoftmax(dim=1))\n",
        "crite=nn.NLLLoss()    #Loss Function\n",
        "logits=model(images)\n",
        "loss=crite(logits,labels)\n",
        "print(loss)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 784])\n",
            "tensor(2.3025, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhS5nYFmz3y3",
        "colab_type": "code",
        "outputId": "6b66c0e7-8399-4e54-ac54-9d5209d443a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "epoch=6\n",
        "optimizer=optim.SGD(model.parameters(),lr=0.01)\n",
        "for i in range(epoch):\n",
        "  runlos=0\n",
        "  for images,labels in trainloader:\n",
        "    images=images.view(images.shape[0],-1)   #flatten the images\n",
        "    optimizer.zero_grad()\n",
        "    output=model.forward(images)\n",
        "    loss=crite(output,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    runlos+=loss.item()\n",
        "  else:\n",
        "    print(f\"Training Loss:{runlos/len(trainloader)}\")\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Loss:1.3148223711356426\n",
            "Training Loss:0.6268670975462969\n",
            "Training Loss:0.5314747961853613\n",
            "Training Loss:0.4859213851281066\n",
            "Training Loss:0.4561746048012268\n",
            "Training Loss:0.4334326731180077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2q1BiSk09wv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def test_network(net, trainloader):\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "    dataiter = iter(trainloader)\n",
        "    images, labels = dataiter.next()\n",
        "\n",
        "    # Create Variables for the inputs and targets\n",
        "    inputs = Variable(images)\n",
        "    targets = Variable(images)\n",
        "\n",
        "    # Clear the gradients from all Variables\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass, then backward pass, then update weights\n",
        "    output = net.forward(inputs)\n",
        "    loss = criterion(output, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def imshow(image, ax=None, title=None, normalize=True):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    image = image.numpy().transpose((1, 2, 0))\n",
        "\n",
        "    if normalize:\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = std * image + mean\n",
        "        image = np.clip(image, 0, 1)\n",
        "\n",
        "    ax.imshow(image)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['left'].set_visible(False)\n",
        "    ax.spines['bottom'].set_visible(False)\n",
        "    ax.tick_params(axis='both', length=0)\n",
        "    ax.set_xticklabels('')\n",
        "    ax.set_yticklabels('')\n",
        "\n",
        "    return ax\n",
        "\n",
        "\n",
        "def view_recon(img, recon):\n",
        "    ''' Function for displaying an image (as a PyTorch Tensor) and its\n",
        "        reconstruction also a PyTorch Tensor\n",
        "    '''\n",
        "\n",
        "    fig, axes = plt.subplots(ncols=2, sharex=True, sharey=True)\n",
        "    axes[0].imshow(img.numpy().squeeze())\n",
        "    axes[1].imshow(recon.data.numpy().squeeze())\n",
        "    for ax in axes:\n",
        "        ax.axis('off')\n",
        "        ax.set_adjustable('box-forced')\n",
        "\n",
        "def view_classify(img, ps, version=\"MNIST\"):\n",
        "    ''' Function for viewing an image and it's predicted classes.\n",
        "    '''\n",
        "    ps = ps.data.numpy().squeeze()\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
        "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
        "    ax1.axis('off')\n",
        "    ax2.barh(np.arange(10), ps)\n",
        "    ax2.set_aspect(0.1)\n",
        "    ax2.set_yticks(np.arange(10))\n",
        "    if version == \"MNIST\":\n",
        "        ax2.set_yticklabels(np.arange(10))\n",
        "    elif version == \"Fashion\":\n",
        "        ax2.set_yticklabels(['T-shirt/top',\n",
        "                            'Trouser',\n",
        "                            'Pullover',\n",
        "                            'Dress',\n",
        "                            'Coat',\n",
        "                            'Sandal',\n",
        "                            'Shirt',\n",
        "                            'Sneaker',\n",
        "                            'Bag',\n",
        "                            'Ankle Boot'], size='small');\n",
        "    ax2.set_title('Class Probability')\n",
        "    ax2.set_xlim(0, 1.1)\n",
        "\n",
        "    plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ7s48a008R4",
        "colab_type": "code",
        "outputId": "da143d1a-8213-4501-c7e6-a0e1db3ec165",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import helper\n",
        "images,labels=next(iter(trainloader))\n",
        "img=images[8].view(1,784)\n",
        "with torch.no_grad():\n",
        "  logits=model.forward(img)\n",
        "p=F.softmax(logits,dim=1)\n",
        "view_classify(img.view(1,28,28),p)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAADsCAYAAAAhDDIOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAa20lEQVR4nO3de5RdZZnn8d+vLkmo3IAU0EASQprI\nCsJCIMNAtzrYQRtQSS91bLDpHns50jccFbsdpnW19m2WPT2y1NG+pJGW9oogOIjIpVsQ7YFIEmgI\nNxeEQBLEQCT3S6Wqnvnj7LTV1ed9UzmcU3vvk+9nrVpU7efsvZ9zUtRz3nc/592OCAEAUDU9ZScA\nAEAzFCgAQCVRoAAAlUSBAgBUEgUKAFBJFCgAQCVRoAB0jO2P2/5S2Xm0wvYXbP9Zi/tmn7ftR22f\nN/6xtufb3mG7t6WkuwwFCsArYvtdtlcWf1h/bPs7tl9bUi5he2eRy0bbV1fxj31EvDoi7mmy/bmI\nmBERI5Jk+x7b/3XSE6wIChSAltm+UtKnJP1PScdImi/pryQtKzGt0yNihqSlkt4l6b3jH2C7b9Kz\nwkGjQAFoie3Zkv5E0u9FxE0RsTMi9kXEtyLiDxL73GD7Bdtbbd9r+9VjYhfZfsz29mL08/vF9kHb\nt9reYvuntr9v+4B/uyLiCUnfl3RqcZx1tv+77Ycl7bTdZ3txMUrZUky7XTzuMIO27ypy+p7tE8bk\n+2nb621vs73K9uvG7TvN9vXFvqttnz5m33W2z2/y+iwoRoF9tv9c0uskfbYYEX7W9udsf3LcPrfY\n/uCBXo86okABaNW5kqZJuvkg9vmOpEWSjpa0WtKXx8Q+L+m3ImKmGkXlu8X2D0naIOkoNUZpfyjp\ngGu02T5FjT/wD47ZfKmkN0s6XJIlfUvSnUU+75P0Zdsnj3n8r0n6U0mDkh4al+8Dkl4j6UhJX5F0\ng+1pY+LLJN0wJv5N2/0Hynu/iPiIGgX2imLa7wpJ10m6dH+Btj0o6fzi+F2HAgWgVXMkvRQRwxPd\nISKujYjtEbFX0sclnV6MxCRpn6RTbM+KiJcjYvWY7cdKOqEYoX0/8ouIrrb9shrF5xpJfz8m9pmI\nWB8RuyWdI2mGpE9ExFBEfFfSrWoUsf2+HRH3Fvl+RNK5tucVz+VLEbE5IoYj4pOSpkoaW9xWRcSN\nEbFP0tVqFPNzJvpaNRMRP5S0VY3pS0m6RNI9EfGTV3LcqqJAAWjVZjWmwCZ0Pcd2r+1P2H7a9jZJ\n64rQYPHft0u6SNKzxXTaucX2v5T0lKQ7ba+1fdUBTnVmRBwRET8fER+NiNExsfVjvj9O0vpx8Wcl\nHd/s8RGxQ9JPi/1k+/dtP15MV26RNHvMcxm/76gao8DjDpD7RFwn6bLi+8skfbENx6wkChSAVt0n\naa+kX5ng49+lxrTX+Wr8MV9QbLckRcQDEbFMjem2b0r6erF9e0R8KCIWSrpY0pW2l6o1Y0dez0ua\nN+561nxJG8f8PG//N7ZnqDFd93xxvenDkt4p6YiIOFyNkY0T+/ZImlucs9V89/uSpGXFNa3FarxW\nXYkCBaAlEbFV0h9J+pztX7E9YLvf9oW2/1eTXWaqUdA2SxpQo/NPkmR7iu1fsz27mBLbJmm0iL3F\n9km2rUYRGNkfe4VWSNol6cNF3udJequkr415zEW2X2t7ihrXou6PiPXFcxmW9KKkPtt/JGnWuOOf\nZfttxQjzA8Vzv/8gc/yJpIVjN0TEBjWuf31R0jeK6cquRIEC0LLi2suVkj6qxh/r9ZKuUPN39f+g\nxhTaRkmP6d//sf51SeuK6b/fVqNBQWo0VfyjpB1qjNr+KiLubkPuQ2oUpAslvaRGe/xvFN1/+31F\n0sfUmNo7Sz+bWrtD0u2SflQ8pz36t9OHkvR/Jf2qpJeL5/a2ovgejE9Leoftl21/Zsz26ySdpi6e\n3pMkc8NCAKgX269XY6rvhAM0jNQaIygAqJGiVf39kq7p5uIkUaAAoDZsL5a0RY22+0+VnE7HMcUH\nAKik7OcX3tjznye3etnpWEUKad/CBcnYE+/7uWRs3p0jyVjvULohKXrTr0nv7vQx+7btSca2Lxrf\nbPQzs+8bf533Z4Y3HmyH7AH0ZNbwjBabtDrwe3LX6A2ZX0wAncIUHwCgkljRFyjR4OBgLFiwoOw0\ngFKtWrXqpYg4avx2ChRQogULFmjlypVlpwGUyvazzbYzxQcAqCQKFACgkiZ/iq/Vzq02d/gNLz0r\nGVt//pRkrP9V25KxRUekO+B+fGK6c27HM7OTsaNOfikZ27R2TjKmWenXa+nJa5KxdTuOTMae/eG5\nydj8O/Y23d77vQebbpckjaa7EAGAERQAoJIoUACASqJAAQAqiTZzoESPbNyqBVd9+xUdY90n3tym\nbIBqYQQFAKgkChQAoJImf4ov11rcYiv5znf8x6bbZ/zuhuQ+6zelb2w58kJ/Mtb3YLol/NmedOyM\nX348GVu1N32+vbcdnYz1/1z6NVl82sZk7J9+eGoyNnVz+mMA+45Pv2abP7ir6fa9V7w6uc/IEzOT\nsQUfvS8Zy/6e5FRkwWEAE8MICmgz2++3vcb2o7Y/UHY+QF1RoIA2sn2qpPdKOlvS6ZLeYvukcrMC\n6okCBbTXYkkrImJXRAxL+p6kt5WcE1BLFCigvdZIep3tObYHJF0kad7YB9i+3PZK2ytHdm0tJUmg\nDvgcFNBGEfG47b+QdKeknZIekjQy7jHLJS2XpKnHLqJzA0hgBAW0WUR8PiLOiojXS3pZ0o/Kzgmo\no2qNoDJtwH0nzEvGnn9r8/bn/v83P7nPMavTK6dvm5+u28OHJUPq252OPXLz4mRs6KThZOy3fvv2\nZOyzq85Lxp7+1s8nYzPT3eIaHkjHZq1Jt8P3bz2i+fb04fTia9OJ7Hpb848OSNLATSsyRy2f7aMj\nYpPt+Wpcfzqn7JyAOqpWgQK6wzdsz5G0T9LvRcSWshMC6ogCBbRZRLyu7ByAbsA1KABAJTGCAkp0\n2vGztZLVyIGmGEEBACqJAgUAqKTaTPE9/9Z0m7m0p+nWKVvSq157NN3S3jOUPpOnpmOHbUofczTz\nSkdvOvjOix5Oxv7P0NJkbKDFXIYH0q9ZT6Y9vXeo+fn2HJ5+D+Q96ZXTN52Z3m/BTek8AHSP2hQo\noBtxR10gjSk+AEAlUaAAAJVEgQLazPYHi5sVrrH9VdvTys4JqCMKFNBGto+X9N8kLYmIUyX1Srqk\n3KyAeqJAAe3XJ+kw232SBiQ9X3I+QC1Vqouvd9asZGzLGeneb29vvmb2jsXpfYYHpqTzyLSZ7z08\n3b6dbdFOL1guZe4IdOuOk5Mx706/v9h9VDqXbJv5jEz7/XD6mLv7muey7aSRptslqWdv+nhDx6f/\nEXqPOToZG/nJpmRsMkTERtv/W9JzknZLujMi7iw1KaCmGEEBbWT7CEnLJJ0o6ThJ021fNu4x3FEX\nmAAKFNBe50t6JiJejIh9km6S9AtjHxARyyNiSUQs6R2YXUqSQB1QoID2ek7SObYHbFvSUkmPl5wT\nUEsUKKCNImKFpBslrZb0iBr/jy0vNSmgpirVJAF0g4j4mKSPlZ0HUHeMoAAAlVSpEdTmZa9Oxpzp\n/e7b3rzOjh6R3qfn9HT31M5N05Mx9WTazEdzq6enYxpNh65+OL1ieW+mzXzHwnR7d/Rm+tozoeHp\nmZXJj26+orz2pfeZ8lR6gYU9c9J5bDlvYTI28/py28wBtE+lChRwqOGOukAaU3wAgEqiQAEAKokp\nPqBE7bhhocRNC9GdGEEBACqpUiOol3450QkmSS+nF3dNLX46mukg2/t8emHaaTvTHXcjU9Jtbs50\nwOVE5m1C7Mp0FGbO1/9y+qC58znTUdibWdx1dNthTbcPzUkfcGhWZmHazc0XAJakF89K5zHz+mQI\nQM0wggIAVBIFCmgj2yfbfmjM1zbbHyg7L6COKjXFB9RdRDwp6TWSZLtX0kZJN5eaFFBTjKCAzlkq\n6emIeLbsRIA6okABnXOJpK+O38gNC4GJoUABHWB7iqSLJd0wPsYNC4GJmfRrUJvfe24yNrojvcBp\nz0im9XtG8/1Onv9Ccp+BhemFZB+7e1Ey1rsnnUfPvmQoK9f2PXRkuk3bmdekL9Mqn80l9xuRaWsf\nPPsnTbdv3zM1uc/259Kt/rmFcHP6Fi5IxobXrmvpmC26UNLqiGj+wgA4IEZQQGdcqibTewAmjgIF\ntJnt6ZLeKOmmsnMB6ow2c6DNImKnpMwdrQBMBCMoAEAlMYICSsQNC4E0RlAAgEqa9BHU4EM7krGd\nx85MxvYenW5B98Bw0+1vOvrx5D6fXfmGZGzW5mRI216VzmNgQ28yllpxXZLUWkd4tq09t7J6ZM4X\n6aeg0b70QTetPqbp9qmLMx9EzZxreFbmIwd70u+rNr7luGTsmM+sS58QQOUwggIAVBLXoIASteuO\nuvtxZ110E0ZQAIBKokABACqJAgW0me3Dbd9o+wnbj9tOL0AJIIlrUED7fVrS7RHxjmJV84GyEwLq\naNILVDzwSDI2/4H0fv4PpyVjT799RtPtVx65NrnPHe9Or6T97B//QjLWN7g7GYuNzfOQpNH+ZEg9\nzbvkX5FWW8md7u7W0Nx0X/viq55puv2k27cl9/n2urOSseP+OZ3H7B80P5ckDb9Q7uLhtmdLer2k\nd0tSRAxJSi+dDyCJKT6gvU6U9KKkv7f9oO1risVjARwkChTQXn2SzpT01xFxhqSdkq4a+wDuqAtM\nDAUKaK8NkjZExIri5xvVKFj/ijvqAhNDgQLaKCJekLTe9snFpqWSHisxJaC26OID2u99kr5cdPCt\nlfSbJecD1BIFCmiziHhI0pKy8wDqrloFyune6Fx7+sJEe/qFHz8nc7I9ycjI1PSq3cM7pyRjfZkJ\n0+zq4rm27+H0a5JrCc8dMye730g6l5GXmi8B/+TZ6QOeNHr/RNP6NzrQlQ+ggrgGBQCopGqNoIBD\nDHfUBdIYQQEAKokCBQCoJKb4gBJxw0IgjREUAKCSunoENbon3Uqek2sJ965MH3auzTzTEj6a+VcY\nzbS89+xrrQW9ZaOZJdKT+7SYSOYjB4rMPxCArsEICgBQSV09ggLKYHudpO2SRiQNRwSrSgAtoEAB\nnfGGiHip7CSAOmOKDwBQSRQooP1C0p22V9m+fHyQGxYCE8MUH9B+r42IjbaPlnSX7Sci4t79wYhY\nLmm5JE09dhEtiUBCtQpURdqHs6uE96dzzK0EHj3p/XJt5sq8JD1DmdXfc2PjXLd4JuZMm3nPtGlN\nt+da/d2XfuIxXN81yyNiY/HfTbZvlnS2pHvzewEYjyk+oI1sT7c9c//3kt4kaU25WQH1VK0RFFB/\nx0i62Y0PGvdJ+kpE3F5uSkA9UaCANoqItZJOLzsPoBswxQcAqCRGUECJuGEhkMYICgBQSfUZQfXk\nerhHD/54Lba0e6S1FcRzbd/OpN+3K3O+3FPInC/XDp99Dn3pRD1zZvNAps08Rjqx5DqAbsEICgBQ\nSfUZQQFdqF131OVOuuhGjKAAAJVEgQIAVBIFCgBQSRQooANs99p+0PatZecC1NXkN0k4s1x2q6uZ\nO1FnR1trYx6ZmglmUozcU8u0dueO2bsns2J57pgtdN5LknOLiE/JHPSIWc23v/hia4nU3/slPS4p\n8cIAOBBGUECb2Z4r6c2Srik7F6DOKFBA+31K0oeVGMdyR11gYihQQBvZfoukTRGxKvWYiFgeEUsi\nYknvwOxJzA6oFwoU0F6/KOli2+skfU3SL9n+UrkpAfVEgQLaKCL+R0TMjYgFki6R9N2IuKzktIBa\nokABACpp8tvMW20lz6xY7t7m/da5Rc49Nd1LPnrM3mSs94X0ftm27xblWtezcquZt9oOP5w+6I7F\nc5puP+xHT2cO2N0i4h5J95ScBlBbjKAAAJXEauZAibijLpDGCAoAUEkUKABAJTHFB5Told6wkBsV\nopsxggIAVFJ9RlCZ9vQYzi3B3ZwXnZiMzZ69Kxnb/cy0zEFba6F3ph0++lpty8+sgp475nCmBz0T\n23JS81+lw9JHA4AsRlAAgEqiQAFtZHua7R/a/hfbj9r+47JzAuqqPlN8QD3slfRLEbHDdr+kH9j+\nTkTcX3ZiQN1QoIA2ioiQtKP4sb/4avFCInBoY4oPaDPbvbYfkrRJ0l0RsaLsnIA6okABbRYRIxHx\nGklzJZ1t+9Sxce6oC0xMfab4nGl/dqLOjo4kd9l+cvpOplteSPd9T8tM1oz0p2PKpZ9Os2W5jvfc\nfNPI1HS0b2t6qfPdxzCLNV5EbLF9t6QLJK0Zs325pOWSNPXYRbxwQAIjKKCNbB9l+/Di+8MkvVHS\nE+VmBdRTfUZQQD0cK+k6271qvAH8ekTcWnJOQC1RoIA2ioiHJZ1Rdh5AN2CKDwBQSRQoAEAlMcUH\nlIg76gJp9SlQmdXMpcxy4Anb56ZbpjWaPtdoi6uLZxYXV2TGsbmVzrMHzejZm95vJNNHn8tl3+C+\n5ueaPj25z+jOnekD5j5WkJP9PQFQJ0zxAQAqqT4jKKALvdI76krcVRfdixEUAKCSKFAAgEqiQAEA\nKokCBbSR7Xm277b9WHFH3feXnRNQV93RJNFCa/FQejHz1uVWLM+l2GILevS2tmR5755coplDttD5\n3TN4ZDKWbzPPPfGD/1jBJBqW9KGIWG17pqRVtu+KiMfKTgyoG0ZQQBtFxI8jYnXx/XZJj0s6vtys\ngHqiQAEdYnuBGgvHrhi3nRsWAhNAgQI6wPYMSd+Q9IGI2DY2FhHLI2JJRCzpHejEXDPQHShQQJvZ\n7lejOH05Im4qOx+grihQQBvZtqTPS3o8Iq4uOx+gzrqji68FewdH0sFcJ1tmjdlsLNc4l2lKy8Wy\nXXW555Bb8DYT6snl2d88GAPT0jtluCf9BGK40gvC/qKkX5f0iO2Him1/GBG3lZgTUEuHbIECOiEi\nfqDs2wMAE8UUHwCgkhhBASXihoVAGiMoAEAlUaAAAJVEgQIAVFJ9rkE51zd98G3HMT3dZu496X7x\n3OKtuRbtbFvXJPd8ZZ9Di3oSbeb7Bmek98kcL0YyHwPoIu24o+5+3FkX3YYRFACgkihQAIBKokAB\nbWT7WtubbK8pOxeg7ihQQHt9QdIFZScBdAMKFNBGEXGvpJ+WnQfQDShQAIBKqk+beZv1TE23Mcfu\nzLLkGdn27Z50D7pHMit3546ZWyE906Xd6qrrvXvSwX0vT226fejw9PNubZ3z+rN9uaTLJal31lEl\nZwNUFyMoYJJxR11gYihQAIBKokABbWT7q5Luk3Sy7Q2231N2TkBdHbLXoIBOiIhLy84B6BaMoAAA\nlUSBAgBUUndP8WVWQI+tU1o65OjUg185XZJ6htK5eLilQ+bbzJsvLn5A0Z+Ojfaln3v0JVYzH0j3\ntB+qbeZjcUddII0RFACgkihQAIBK6u4pPqDi2nXDQm5WiG7ECAoAUEkUKABAJVGgAACV1NXXoNyb\nbnGOKek+bO9O1+3e3ZnW9dwi6Jm27+x+mVby3MrjznXDZ2Ij03L96Qf/fmZkaibJLmX7AkmfltQr\n6ZqI+ETJKQG1xAgKaCPbvZI+J+lCSadIutT2KeVmBdQTBQpor7MlPRURayNiSNLXJC0rOSeglihQ\nQHsdL2n9mJ83FNv+le3Lba+0vXJk19ZJTQ6oEwoUMMm4YSEwMRQooL02Spo35ue5xTYAB4kCBbTX\nA5IW2T7R9hRJl0i6peScgFqqT5t5HPwq4jGcXiZ8+pxdydiubel1tvfNzJxwJLNi+e50L3lvpq09\nqyezunhmN2fyzLW1jxyWbkHvnzXUdPueOZnl0XOceU1ipLVjToKIGLZ9haQ71GgzvzYiHi05LaCW\n6lOggJqIiNsk3VZ2HkDdMcUHAKgkRlBAibhhIZDGCAoAUEkUKABAJVGgAACVdMheg5r7zieTsb3n\nn5GMbVmYbpveNyt9vtEWu61HM/9CuVXQI/PWw5kFy3uG0jv2b0/3oE9dM9B0+zHfSL/O1W0WB1AF\njKAAAJVEgQIAVBIFCgBQSRQoAEAlHbJNEkAVrFq1aoftdCfJ5BuU9FLZSRTIpbluzOWEZhspUEC5\nnoyIJWUnsZ/tlVXJh1yaO5RyyRaou0ZvyKxtDUzAX5edAIC64hoUAKCSKFBAuZaXncA4VcqHXJo7\nZHJxtHAjQAAAOo0RFACgkihQwCSwfYHtJ20/ZfuqJvGptq8v4itsLygxlyttP2b7Ydv/ZLtpC/Bk\n5DLmcW+3HbY72r02kXxsv7N4fR61/ZWycrE93/bdth8s/q0u6lAe19reZHtNIm7bnynyfNj2mW07\neUTwxRdfHfyS1CvpaUkLJU2R9C+SThn3mN+V9DfF95dIur7EXN4gaaD4/nfKzKV43ExJ90q6X9KS\nkv+dFkl6UNIRxc9Hl5jLckm/U3x/iqR1Hcrl9ZLOlLQmEb9I0nckWdI5kla069yMoIDOO1vSUxGx\nNiKGJH1N0rJxj1km6bri+xslLbXdiY95HDCXiLg7InYVP94vaW4H8phQLoU/lfQXkvZ0KI+Dyee9\nkj4XES9LUkRsKjGXkLT/HgqzJT3fiUQi4l5JP808ZJmkf4iG+yUdbvvYdpybAgV03vGS1o/5eUOx\nreljImJY0lZJc0rKZaz3qPHuuBMOmEsxXTQvIr7doRwOKh9Jr5L0Ktv/bPt+2xeUmMvHJV1me4Ok\n2yS9r0O5HMjB/k5NGCtJAGjK9mWSlkj6TyWdv0fS1ZLeXcb5E/rUmOY7T42R5b22T4uILSXkcqmk\nL0TEJ22fK+mLtk+NiMwd3+qFERTQeRslzRvz89xiW9PH2O5TY8pmc0m5yPb5kj4i6eKI2NuBPCaS\ny0xJp0q6x/Y6Na5v3NLBRomJvDYbJN0SEfsi4hlJP1KjYJWRy3skfV2SIuI+SdPUWBtvsk3od6oV\nFCig8x6QtMj2ibanqNEEccu4x9wi6b8U379D0nejuAI92bnYPkPS36pRnDp1jeWAuUTE1ogYjIgF\nEbFAjethF0fEyjLyKXxTjdGTbA+qMeW3tqRcnpO0tMhlsRoF6sUO5HIgt0j6jaKb7xxJWyPix+04\nMFN8QIdFxLDtKyTdoUZ31rUR8ajtP5G0MiJukfR5NaZonlLjgvQlJebyl5JmSLqh6NN4LiIuLimX\nSTPBfO6Q9Cbbj0kakfQHEdH2ke4Ec/mQpL+z/UE1Gibe3Yk3Nba/qkZRHiyud31MUn+R59+ocf3r\nIklPSdol6Tfbdu7OvEkDAOCVYYoPAFBJFCgAQCVRoAAAlUSBAgBUEgUKAFBJFCgAQCVRoAAAlUSB\nAgBU0v8HQ1xh+1/VEC8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OczibIGJQCAd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "16a7e623-c207-4da6-a5c9-40eb28a6bcdd"
      },
      "source": [
        "images,labels=next(iter(testloader))\n",
        "images=images.view(images.shape[0],-1) \n",
        "ps=torch.exp(model(images))\n",
        "topp,topclass=ps.topk(1,dim=1)\n",
        "equals=topclass==labels.view(*topclass.shape)\n",
        "print(equals)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [ True],\n",
            "        [ True],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [ True],\n",
            "        [ True],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [ True],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [ True],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [ True],\n",
            "        [False],\n",
            "        [ True],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elYZUjm8TTKm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "d2e43cbb-39f9-4c73-fd5e-c3ad0a163276"
      },
      "source": [
        "#validation Solution\n",
        "from torch import optim\n",
        "\n",
        "epoch=6\n",
        "optimizer=optim.SGD(model.parameters(),lr=0.01)\n",
        "for i in range(epoch):\n",
        "  runlos=0\n",
        "  for images,labels in trainloader:\n",
        "    images=images.view(images.shape[0],-1)   #flatten the images\n",
        "    optimizer.zero_grad()\n",
        "    output=model.forward(images)\n",
        "    loss=crite(output,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    runlos+=loss.item()\n",
        "  else:\n",
        "    testloss=0\n",
        "    acc=0\n",
        "    with torch.no_grad():\n",
        "      for images,labels in trainloader:\n",
        "        images=images.view(images.shape[0],-1) \n",
        "        logps=model(images)\n",
        "        testloss+=crite(logps,labels)\n",
        "        ps=torch.exp(logps)\n",
        "        topp,topclass=ps.topk(1,dim=1)\n",
        "        equals= topclass==labels.view(*topclass.shape)\n",
        "        acc+=torch.mean(equals.type(torch.FloatTensor))\n",
        "    print(f\"Training Loss:{runlos/len(trainloader)}\")\n",
        "    print(f\"Test Loss:{testloss/len(testloader)}\")\n",
        "    print(acc/len(testloader))\n",
        "\n",
        "        \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Loss:0.3614298595326033\n",
            "Test Loss:2.161282539367676\n",
            "tensor(5.2006)\n",
            "Training Loss:0.3512072272535199\n",
            "Test Loss:2.100214958190918\n",
            "tensor(5.2094)\n",
            "Training Loss:0.3412096968321785\n",
            "Test Loss:2.106407403945923\n",
            "tensor(5.2100)\n",
            "Training Loss:0.33246508178743983\n",
            "Test Loss:1.9152727127075195\n",
            "tensor(5.2948)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}